{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 1) DATA ACQUISITON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import datetime as dt # use datetime to specify dates for the Pandas datareade\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "#from pandas_datareader import data as pdr #new\n",
    "import pickle\n",
    "import requests\n",
    "#import fix_yahoo_finance #new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : web scrapping in wiki to get tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE A) To get the tickers of the 100 companies\n",
    "\n",
    "def save_nasdaq100_tickers():\n",
    "    resp = requests.get('https://en.wikipedia.org/wiki/NASDAQ-100')\n",
    "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[1].text\n",
    "        tickers.append(ticker.rstrip())\n",
    "        \n",
    "    with open(\"nasdaq100tickers.pickle\",\"wb\") as f:\n",
    "        pickle.dump(tickers,f)\n",
    "        \n",
    "    return tickers\n",
    "\n",
    "#save_nasdaq100_tickers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pull Data from Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_yahoo(reload_nasdaq100=False):\n",
    "    if reload_nasdaq100:\n",
    "        tickers = save_nasdaq100_tickers()\n",
    "    else:\n",
    "        with open(\"nasdaq100tickers.pickle\", \"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    if not os.path.exists('stock_dfs'):\n",
    "        os.makedirs('stock_dfs')\n",
    "\n",
    "    start = dt.datetime(2010, 1, 1)\n",
    "    end = dt.datetime.now()\n",
    "    for ticker in tickers:\n",
    "        if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):\n",
    "            df = web.DataReader(ticker, 'yahoo', start, end)\n",
    "            df.to_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        else:\n",
    "            print('Already have {}'.format(ticker))\n",
    "\n",
    "get_data_from_yahoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Join 100 companies in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "                 ATVI       ADBE   AMD       ALXN   ALGN       GOOGL  \\\n",
      "Date                                                                   \n",
      "2010-01-04  10.193225  37.090000  9.70  24.125000  18.50  313.688690   \n",
      "2010-01-05  10.211267  37.700001  9.71  23.780001  18.01  312.307312   \n",
      "2010-01-06  10.157144  37.619999  9.57  23.840000  17.48  304.434448   \n",
      "2010-01-07   9.913588  36.889999  9.47  23.930000  17.43  297.347351   \n",
      "2010-01-08   9.832404  36.689999  9.43  23.975000  17.66  301.311310   \n",
      "\n",
      "                  GOOG        AMZN       AAL       AMGN    ...           VRSN  \\\n",
      "Date                                                       ...                  \n",
      "2010-01-04  311.349976  133.899994  4.557168  48.350658    ...      21.007998   \n",
      "2010-01-05  309.978882  134.690002  5.073073  47.931801    ...      21.228777   \n",
      "2010-01-06  302.164703  132.250000  4.862889  47.571617    ...      20.999506   \n",
      "2010-01-07  295.130463  130.000000  5.006195  47.136017    ...      20.710794   \n",
      "2010-01-08  299.064880  133.520004  4.910658  47.554852    ...      20.872135   \n",
      "\n",
      "                 VRSK       VRTX        WBA        WDC       WLTW  WDAY  \\\n",
      "Date                                                                      \n",
      "2010-01-04  29.792044  44.240002  30.470020  37.029556  63.170490   NaN   \n",
      "2010-01-05  29.991653  42.779999  30.224958  38.011318  63.028927   NaN   \n",
      "2010-01-06  30.380898  42.029999  29.996222  38.044056  64.043610   NaN   \n",
      "2010-01-07  30.330994  41.500000  30.175928  37.299545  63.854847   NaN   \n",
      "2010-01-08  29.941751  40.669998  30.216782  37.487709  63.784065   NaN   \n",
      "\n",
      "                 WYNN        XEL       XLNX  \n",
      "Date                                         \n",
      "2010-01-04  46.595161  14.790739  20.349150  \n",
      "2010-01-05  49.429043  14.615329  20.092579  \n",
      "2010-01-06  48.780678  14.643392  19.956280  \n",
      "2010-01-07  49.822430  14.580243  19.755833  \n",
      "2010-01-08  49.465473  14.587262  20.044474  \n",
      "\n",
      "[5 rows x 103 columns]\n"
     ]
    }
   ],
   "source": [
    "# To join all the tickers csv files in one , taking just the colum Adj Close which is the one what interested us to perform the \n",
    "# analysis \n",
    "\n",
    "def compile_data():\n",
    "    with open(\"nasdaq100tickers.pickle\", \"rb\") as f:\n",
    "        tickers = pickle.load(f)\n",
    "\n",
    "    main_df = pd.DataFrame()\n",
    "\n",
    "    for count, ticker in enumerate(tickers):\n",
    "        df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        df.set_index('Date', inplace=True)\n",
    "\n",
    "        df.rename(columns={'Adj Close': ticker}, inplace=True)\n",
    "        df.drop(['Open', 'High', 'Low', 'Close', 'Volume'], 1, inplace=True)\n",
    "\n",
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df, how='outer')\n",
    "\n",
    "        if count % 10 == 0:\n",
    "            print(count)\n",
    "    print(main_df.head())\n",
    "    main_df.to_csv('nasdaq100_joined_closes.csv')\n",
    "\n",
    "\n",
    "compile_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
